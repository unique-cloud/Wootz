from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

slim = tf.contrib.slim


#####################################################################
####### the function below are generated by the Wootz compiler ######
#####################################################################
def inception_v2(inputs,
                 num_classes=1000,
                 is_training=True,
                 reuse=None,
                 scope='InceptionV2',
                 config=None):


  ############## template code added for multiplexing ##############
  # calculate the number of filter in a conv given config 
  selectdepth = lambda k,v: int(config[k]['ratio']*v) if config and k in config and 'ratio' in config[k] else v 

  # select the input tensor to a module 
  selectinput = lambda k, v: config[k]['input'] if config and k in config and 'input' in config[k] else v 
  ############## end template code ##############

  with tf.variable_scope(scope, "Model", reuse=reuse):
    with slim.arg_scope(default_arg_scope(is_training)):
      
      end_points = {}
      
      end_point = 'conv1_7x7'
        net = slim.conv2d(inputs, num_classes, [7, 7], activation_fn=None,
                             normalizer_fn=None, scope='"conv1_7x7"')
      end_points[end_point] = net

      end_point = 'conv1_7x7_increase'
      with tf.variable_scope(end_point):
        conv1_7x7_increase = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                             normalizer_fn=None, scope='"conv1_7x7_increase"')
        end_points[end_point] = conv1_7x7_increase

      end_point = 'pool1_3x3_s2'
      net = slim.max_pool2d(conv1_7x7_increase, [3, 3], stride=2, scope=end_point)
      end_points[end_point] = net

      end_point = 'conv2_3x3_reduce'
      net = slim.conv2d(net, 64, [1, 1], scope=end_point)
      end_points[end_point] = net

      end_point = 'conv2_3x3'
      net = slim.conv2d(net, 192, [3, 3], scope=end_point)
      end_points[end_point] = net

      end_point = 'pool2_3x3_s2'
      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_a1_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 64, [1, 1], scope='inception_a1_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='inception_a1_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 64, [3, 3], scope='inception_a1_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='inception_a1_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,96), [3, 3], scope='inception_a1_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='inception_a1_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_a1_pool')
            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='inception_a1_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_a2_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 64, [1, 1], scope='inception_a2_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='inception_a2_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='inception_a2_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='inception_a2_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,96), [3, 3], scope='inception_a2_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='inception_a2_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_a2_pool')
            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='inception_a2_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'reduction_a_concat'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, selectdepth(end_point,128), [1, 1], scope='reduction_a_3x3_a_reduce')
            branch_0 = slim.conv2d(branch_0, 160, [3, 3], stride=2, scope='reduction_a_3x3_a')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='reduction_a_3x3_b_reduce')
            branch_1 = slim.conv2d(branch_1, selectdepth(end_point,96), [3, 3], scope='reduction_a_3x3_b1')
            branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, scope='reduction_a_3x3_b2')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.max_pool2d(net, [3, 3], stride=2, scope='reduction_a_pool')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_b1_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 224, [1, 1], scope='inception_b1_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,64), [1, 1], scope='inception_b1_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='inception_b1_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,96), [1, 1], scope='inception_b1_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,128), [3, 3], scope='inception_b1_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='inception_b1_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_b1_pool')
            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='inception_b1_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_b2_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 192, [1, 1], scope='inception_b2_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,96), [1, 1], scope='inception_b2_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope='inception_b2_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,96), [1, 1], scope='inception_b2_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,128), [3, 3], scope='inception_b2_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='inception_b2_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_b2_pool')
            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='inception_b2_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_b3_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 160, [1, 1], scope='inception_b3_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,128), [1, 1], scope='inception_b3_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 160, [3, 3], scope='inception_b3_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,128), [1, 1], scope='inception_b3_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,160), [3, 3], scope='inception_b3_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 160, [3, 3], scope='inception_b3_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_b3_pool')
            branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='inception_b3_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_b4_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 96, [1, 1], scope='inception_b4_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,128), [1, 1], scope='inception_b4_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope='inception_b4_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,160), [1, 1], scope='inception_b4_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,192), [3, 3], scope='inception_b4_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 192, [3, 3], scope='inception_b4_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_b4_pool')
            branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='inception_b4_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'reduction_b_concat'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, selectdepth(end_point,128), [1, 1], scope='reduction_b_3x3_a_reduce')
            branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2, scope='reduction_b_3x3_a')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,192), [1, 1], scope='reduction_b_3x3_b_reduce')
            branch_1 = slim.conv2d(branch_1, selectdepth(end_point,256), [3, 3], scope='reduction_b_3x3_b1')
            branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2, scope='reduction_b_3x3_b2')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.max_pool2d(net, [3, 3], stride=2, scope='reduction_b_pool')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_c1_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 352, [1, 1], scope='inception_c1_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,192), [1, 1], scope='inception_c1_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='inception_c1_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,160), [1, 1], scope='inception_c1_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,224), [3, 3], scope='inception_c1_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 224, [3, 3], scope='inception_c1_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_c1_pool')
            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='inception_c1_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      ############## template code added for multiplexing ##############
      net = selectinput(end_point, net)
      ############## end template code ##############
      end_point = 'inception_c2_output'
      with tf.variable_scope(end_point):
        with tf.variable_scope('branch_0'):
            branch_0 = slim.conv2d(net, 352, [1, 1], scope='inception_c2_1x1')
        with tf.variable_scope('branch_1'):
            branch_1 = slim.conv2d(net, selectdepth(end_point,192), [1, 1], scope='inception_c2_3x3_a_reduce')
            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='inception_c2_3x3_a')
        with tf.variable_scope('branch_2'):
            branch_2 = slim.conv2d(net, selectdepth(end_point,192), [1, 1], scope='inception_c2_3x3_b_reduce')
            branch_2 = slim.conv2d(branch_2, selectdepth(end_point,224), [3, 3], scope='inception_c2_3x3_b1')
            branch_2 = slim.conv2d(branch_2, 224, [3, 3], scope='inception_c2_3x3_b2')
        with tf.variable_scope('branch_3'):
            branch_3 = slim.avg_pool2d(net, [3, 3], stride=1, scope='inception_c2_pool')
            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='inception_c2_pool_proj')
        net = tf.concat(
            axis=3, values=[branch_0, branch_1, branch_2, branch_3])
      end_points[end_point] = net

      end_point = 'pool_7x7_s1'
      with tf.variable_scope(end_point):
        pool_7x7_s1 = slim.dropout(pool_7x7_s1, 0.800000, scope='"pool_7x7_s1_drop"')
        end_points[end_point] = pool_7x7_s1

      end_point = 'classifier'
      net = slim.fully_connected(pool_7x7_s1, 1000, scope=end_point)
      end_points[end_point] = net

      end_point = 'prob'
      net = slim.softmax(net, scope=end_point)
      end_points[end_point] = net

  return net, end_points

### change the default image_size based on the input image size specified in prototxt ###  
inception_v2.default_image_size = 231


##############################################################################
########## below are template code hard-coded in compiler ####################
##############################################################################
# The code is applicable to any model. It is adapted from 
# https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_utils.py
def default_arg_scope(is_training=True, 
                        weight_decay=0.00004,
                        use_batch_norm=True,
                        batch_norm_decay=0.9997,
                        batch_norm_epsilon=0.001,
                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):

  batch_norm_params = {
      # Decay for the moving averages.
      'decay': batch_norm_decay,
      # epsilon to prevent 0s in variance.
      'epsilon': batch_norm_epsilon,
      # collection containing update_ops.
      'updates_collections': batch_norm_updates_collections,
      # use fused batch norm if possible.
      'fused': None,
  }
  if use_batch_norm:
    normalizer_fn = slim.batch_norm
    normalizer_params = batch_norm_params
  else:
    normalizer_fn = None
    normalizer_params = {}

  # Set training state 
  with slim.arg_scope([slim.batch_norm, slim.dropout],
                        is_training=is_training):
    # Set weight_decay for weights in Conv and FC layers.
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        weights_regularizer=slim.l2_regularizer(weight_decay)):
      # Set batch norm 
      with slim.arg_scope(
          [slim.conv2d],
          normalizer_fn=normalizer_fn,
          normalizer_params=normalizer_params):
          # Set default padding and stride
            with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                      stride=1, padding='SAME') as sc:
              return sc